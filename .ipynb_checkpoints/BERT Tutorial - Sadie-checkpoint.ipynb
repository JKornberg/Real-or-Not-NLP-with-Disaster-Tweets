{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for code: https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pytorch/pytorch/issues/19406#issuecomment-581178550\n",
    "#pip install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tuples: 8,551\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6363</th>\n",
       "      <td>d_98</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John kissed even the ugliest woman.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6733</th>\n",
       "      <td>m_02</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nurse Rooke suspected that Mrs Clay planned to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>c_13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zeke cooked and ate the chili.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>sks13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We put a book on the table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4728</th>\n",
       "      <td>ks08</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Seoul was slept in by the businessman last night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>r-67</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Willy is taller than Bill by as much as it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6766</th>\n",
       "      <td>m_02</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>The person who never had he been so offended w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>bc01</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>The problem's perception is quite thorough.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6610</th>\n",
       "      <td>m_02</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>With which club did you hit the winning putt?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8545</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Anson thought that himself was going to the club.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "6363            d_98      1         NaN   \n",
       "6733            m_02      1         NaN   \n",
       "5631            c_13      1         NaN   \n",
       "7498           sks13      1         NaN   \n",
       "4728            ks08      0           *   \n",
       "1786            r-67      1         NaN   \n",
       "6766            m_02      0           *   \n",
       "982             bc01      0           *   \n",
       "6610            m_02      1         NaN   \n",
       "8545            ad03      0           *   \n",
       "\n",
       "                                               sentence  \n",
       "6363                John kissed even the ugliest woman.  \n",
       "6733  Nurse Rooke suspected that Mrs Clay planned to...  \n",
       "5631                     Zeke cooked and ate the chili.  \n",
       "7498                        We put a book on the table.  \n",
       "4728  Seoul was slept in by the businessman last night.  \n",
       "1786  Willy is taller than Bill by as much as it is ...  \n",
       "6766  The person who never had he been so offended w...  \n",
       "982         The problem's perception is quite thorough.  \n",
       "6610      With which club did you hit the winning putt?  \n",
       "8545  Anson thought that himself was going to the club.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/cola_public/raw/in_domain_train.tsv',\n",
    "                delimiter='\\t',\n",
    "                header = None,\n",
    "                names = ['sentence_source','label','label_notes','sentence'])\n",
    "print('Number of tuples: {:,}'.format(df.shape[0]))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
    "input_ids = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_sent = tokenizer.encode(sent,\n",
    "                                    add_special_tokens=True,\n",
    "                                    truncation=True,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    max_length=64)\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "print('Original:', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 64\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length:', max([len(s) for s in input_ids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "\n",
    "for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Test Set Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels,\n",
    "                                                                                    random_state = 2018,\n",
    "                                                                                    test_size = 0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks,\n",
    "                                                       labels,\n",
    "                                                       random_state = 2018,\n",
    "                                                       test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2002, 2939, 1996, 3328, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs[0])\n",
    "print(train_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readying BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:transformers.modeling_utils:Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorflow\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                     num_labels=2,\n",
    "                                                     output_attentions=False,\n",
    "                                                     output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT Model has 201 parameters\n",
      "=====Embedding Layer=====\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "=====First Transformer=====\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "=====Output Layer=====\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "print('The BERT Model has {:} parameters'.format(len(params)))\n",
    "\n",
    "print('=====Embedding Layer=====')\n",
    "for p in params[0:5]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
    "print('=====First Transformer=====')\n",
    "for p in params[5:21]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
    "print('=====Output Layer=====')\n",
    "for p in params[-4:]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test accuracy\n",
    "import numpy as np\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Formatting times\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    40 of   241. Elapsed: 0:00:00.\n",
      "Average Training Loss: 0.00\n",
      "Training Epoch Took: 0:00:06\n",
      "=====Validation=====\n",
      "Batch    80 of   241. Elapsed: 0:00:06.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:13\n",
      "=====Validation=====\n",
      "Batch   120 of   241. Elapsed: 0:00:13.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:20\n",
      "=====Validation=====\n",
      "Batch   160 of   241. Elapsed: 0:00:20.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:27\n",
      "=====Validation=====\n",
      "Batch   200 of   241. Elapsed: 0:00:27.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:34\n",
      "=====Validation=====\n",
      "Batch   240 of   241. Elapsed: 0:00:34.\n",
      "Average Training Loss: 0.02\n",
      "Training Epoch Took: 0:00:39\n",
      "=====Validation=====\n",
      "Batch    40 of   241. Elapsed: 0:00:00.\n",
      "Average Training Loss: 0.00\n",
      "Training Epoch Took: 0:00:06\n",
      "=====Validation=====\n",
      "Batch    80 of   241. Elapsed: 0:00:06.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:13\n",
      "=====Validation=====\n",
      "Batch   120 of   241. Elapsed: 0:00:13.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:19\n",
      "=====Validation=====\n",
      "Batch   160 of   241. Elapsed: 0:00:19.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:26\n",
      "=====Validation=====\n",
      "Batch   200 of   241. Elapsed: 0:00:26.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:32\n",
      "=====Validation=====\n",
      "Batch   240 of   241. Elapsed: 0:00:32.\n",
      "Average Training Loss: 0.02\n",
      "Training Epoch Took: 0:00:36\n",
      "=====Validation=====\n",
      "Batch    40 of   241. Elapsed: 0:00:00.\n",
      "Average Training Loss: 0.00\n",
      "Training Epoch Took: 0:00:06\n",
      "=====Validation=====\n",
      "Batch    80 of   241. Elapsed: 0:00:07.\n",
      "Average Training Loss: 0.00\n",
      "Training Epoch Took: 0:00:13\n",
      "=====Validation=====\n",
      "Batch   120 of   241. Elapsed: 0:00:13.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:20\n",
      "=====Validation=====\n",
      "Batch   160 of   241. Elapsed: 0:00:20.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:26\n",
      "=====Validation=====\n",
      "Batch   200 of   241. Elapsed: 0:00:26.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:33\n",
      "=====Validation=====\n",
      "Batch   240 of   241. Elapsed: 0:00:33.\n",
      "Average Training Loss: 0.01\n",
      "Training Epoch Took: 0:00:36\n",
      "=====Validation=====\n",
      "Batch    40 of   241. Elapsed: 0:00:00.\n",
      "Average Training Loss: 0.00\n",
      "Training Epoch Took: 0:00:07\n",
      "=====Validation=====\n",
      "Batch    80 of   241. Elapsed: 0:00:07.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    t0 = time.time() # start time\n",
    "    total_loss = 0\n",
    "    model.train() # puts model into training mode\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0: # show progress every 40 batches\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            b_input_ids = batch[0]\n",
    "            b_input_mask = batch[1]\n",
    "            b_labels = batch[2]\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids = None,\n",
    "                            attention_mask = b_input_mask,\n",
    "                            labels = b_labels)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "            loss_values.append(avg_train_loss)\n",
    "\n",
    "    print('Average Training Loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('Training Epoch Took: {:}'.format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
